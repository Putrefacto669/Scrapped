require 'nokogiri'
require 'httparty'
require 'csv'
require 'json'

class WebScraper
  def initialize
    @data = []
  end

  def scrape_news
    puts "üì∞ Scrapeando noticias de ejemplo..."

    # Datos de ejemplo (evitamos problemas de conexi√≥n)
    sample_news = [
      {
        id: 1,
        title: "Ruby 3.2 libera nuevas caracter√≠sticas de performance",
        link: "https://www.ruby-lang.org/news",
        source: "Ruby News",
        scraped_at: Time.now.strftime("%Y-%m-%d %H:%M:%S")
      },
      {
        id: 2,
        title: "Rails 7 introduce nuevas herramientas de desarrollo",
        link: "https://rubyonrails.org/news",
        source: "Rails News",
        scraped_at: Time.now.strftime("%Y-%m-%d %H:%M:%S")
      },
      {
        id: 3,
        title: "GitHub Copilot ahora soporta Ruby y Rails",
        link: "https://github.com/features/copilot",
        source: "GitHub News",
        scraped_at: Time.now.strftime("%Y-%m-%d %H:%M:%S")
      }
    ]

    @data = sample_news
    puts "‚úÖ #{@data.size} noticias cargadas"
  end

  def scrape_real_website
    puts "üåê Intentando scrapear sitio web real..."

    begin
      # Un sitio simple y confiable
      url = "https://www.wikipedia.org/"
      response = HTTParty.get(url, timeout: 10)

      if response.success?
        doc = Nokogiri::HTML(response.body)
        titles = doc.css('h1, h2, h3')

        titles.each_with_index do |title, index|
          @data << {
            id: index + 1,
            title: title.text.strip,
            tag: title.name,
            source: "https.org",
            scraped_at: Time.now.strftime("%Y-%m-%d %H:%M:%S")
          }
        end

        puts "‚úÖ #{titles.size} elementos encontrados"
      else
        puts "‚ùå Error en la respuesta HTTP"
        load_sample_data
      end

    rescue StandardError => e
      puts "‚ùå Error al conectar: #{e.message}"
      load_sample_data
    end
  end

  def save_to_csv(filename = "scraped_data.csv")
    return if @data.empty?

    CSV.open(filename, "w") do |csv|
      csv << @data.first.keys
      @data.each { |item| csv << item.values }
    end

    puts "üíæ Datos guardados en #{filename}"
  end

  def save_to_json(filename = "scraped_data.json")
    return if @data.empty?

    File.write(filename, JSON.pretty_generate(@data))
    puts "üíæ Datos guardados en #{filename}"
  end

  def display_data
    if @data.empty?
      puts "üì≠ No hay datos para mostrar"
      return
    end

    puts "\n" + "="*50
    puts "üìä DATOS EXTRA√çDOS (#{@data.size} registros)"
    puts "="*50

    @data.each do |item|
      item.each { |key, value| puts "  #{key}: #{value}" }
      puts "-" * 30
    end
  end

  def clear_data
    @data.clear
    puts "üóëÔ∏è  Datos limpiados"
  end

  private

  def load_sample_data
    puts "üìù Cargando datos de ejemplo..."

    @data = [
      {
        id: 1,
        title: "Ejemplo de noticia 1 - Aprendiendo Ruby",
        link: "https://ejemplo.com/noticia1",
        source: "Ejemplo",
        scraped_at: Time.now.strftime("%Y-%m-%d %H:%M:%S")
      },
      {
        id: 2,
        title: "Ejemplo de noticia 2 - Web Scraping con Nokogiri",
        link: "https://ejemplo.com/noticia2",
        source: "Ejemplo",
        scraped_at: Time.now.strftime("%Y-%m-%d %H:%M:%S")
      }
    ]
  end
end

# üöÄ INTERFAZ MEJORADA - M√ÅS ROBUSTA
def main
  scraper = WebScraper.new

  puts "üåê WEB SCRAPER CON RUBY"
  puts "=" * 40

  begin
    loop do
      puts "\n¬øQu√© quieres hacer?"
      puts "1. üì∞ Scrapear noticias de ejemplo"
      puts "2. üåê Scrapear sitio web real (httpbin.org)"
      puts "3. üëÄ Ver datos actuales"
      puts "4. üíæ Guardar a CSV"
      puts "5. üíæ Guardar a JSON"
      puts "6. üóëÔ∏è  Limpiar datos"
      puts "7. üö™ Salir"
      print "\nSelecciona una opci√≥n (1-7): "

      # Manejo m√°s robusto de la entrada
      input = gets
      break if input.nil? # Si se cierra la entrada

      option = input.chomp.to_i

      case option
      when 1
        scraper.scrape_news
      when 2
        scraper.scrape_real_website
      when 3
        scraper.display_data
      when 4
        scraper.save_to_csv
      when 5
        scraper.save_to_json
      when 6
        scraper.clear_data
      when 7
        puts "üëã ¬°Hasta luego!"
        break
      else
        puts "‚ùå Opci√≥n no v√°lida. Por favor usa 1-7."
      end
    end
  rescue Interrupt
    puts "\n\n‚èπÔ∏è  Programa interrumpido por el usuario"
  rescue StandardError => e
    puts "‚ùå Error inesperado: #{e.message}"
  end
end

# Ejecutar el programa
if __FILE__ == $0
  main
end
